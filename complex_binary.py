# -*- coding: utf-8 -*-
"""complex_binary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sP28NFmRjlUb5csDeuBFK6evKcxxIBzl
"""

import os
import torch
from torch import nn
import pandas as pd
import numpy as np
# import matplotlib.pyplot as plt
import argparse

from models import NeuralNetwork
from utils import ImageFolder
# from metrics import F1, TSS

parser = argparse.ArgumentParser(description='complex_binary')
parser.add_argument('--epoch', type=int, default=100, help='number of epoches')
parser.add_argument('--lr', type=float, default=0.0001, help='learning rate')
# parser.add_argument('--batch_size', type=int, default=32, help='batch size')
# parser.add_argument('--use_gpu', dest='use_gpu', action='store_true', default=True, help='use gpu')
parser.add_argument('--use_benchmark', dest='use_benchmark', default=True, help='use benchmark')
# parser.add_argument('--exp_name', type=str, default='cudnn_test', help='output file name')
args = parser.parse_args()


print(args)

base = "/scratch/xl73/"
LR = args.lr

""" Check Device
1.   Add support to one cuda device
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))

# Use bench mark to accelerate the training
if device == 'cuda' and args.use_benchmark:
    torch.backends.cudnn.benchmark = True

"""### Load Data

1.   Create customized dataset 

"""

from torch.utils.data import Dataset, DataLoader, random_split

# data_transforms = transforms.Compose([
#         transforms.ToTensor(),
# ])

class SolarData(Dataset):
    """Solar dataset."""

    def __init__(self, data1, data2, log = True):
        """
        Args:
            data (np.array): Path to the npz file with annotations.
        """
        super(SolarData, self).__init__()
        self.src = np.concatenate((data1["arr_0"], data2["arr_0"]), axis = 0)
        self.tar = np.concatenate((data1["arr_1"], data2["arr_1"]), axis = 0)

        # Everything smaller than 0 is wrong
        self.src[self.src <= 1] = 1
        if log:
            self.src = np.log(self.src)
        self.src = self.src.reshape(self.src.shape[0], 3, 256, 256)
        self.tar = self.tar.reshape(self.tar.shape[0], 1)

        # CenterCrop
        self.src = self.src[:, :, 26:230, 26:230]


    def __len__(self):
        return len(self.tar)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        sample = [self.src[idx-1], self.tar[idx-1]]
        # sample = data_transforms(sample) # convert to tensor

        return sample

# data1 = np.load(base + 'maps_256_6800_flares.npz')
# data2 = np.load(base + 'maps_256_7000_non_flares.npz')

# dataset = SolarData(data1 = data1, data2 = data2)
# train_size = len(dataset) * 4 // 5
# val_size = len(dataset) - train_size

# print(len(dataset), train_size, val_size)
# solar_dataset, valid_dataset = random_split(dataset, [train_size, val_size])

# del data1.f
# del data2.f
# data1.close()
# data2.close()



import torchvision.transforms as transforms

traindir = base + 'train'
valdir = base + 'val'

train_dataset = ImageFolder(
        traindir,
        transforms.Compose([
            transforms.CenterCrop(204),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
        ]))
valid_dataset = ImageFolder(
        valdir,
        transforms.Compose([
            transforms.CenterCrop(204),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
        ]))

trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64,
                                          shuffle=True, num_workers=2, pin_memory=True)
validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=128,
                                          shuffle=True, num_workers=2, pin_memory=True)



"""### Create Model

"""

model = NeuralNetwork(5).to(device)
print(model)


#f1 = F1().cuda()
#tss = TSS().cuda()

import torch.optim as optim

criterion = nn.BCEWithLogitsLoss() # This combines Sigmoid and BCE
optimizer = optim.Adam(model.parameters(), lr=LR)

"""### Trainning"""

min_valid_loss = -np.inf
EPOCH = args.epoch
train_loss_list = []
valid_loss_list = []

for epoch in range(EPOCH):  # loop over the dataset multiple times

    train_loss = 0.0
    model.train()
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # Convert to float
        if device == "cpu":
            inputs = inputs.float()
            labels = labels.float()
        else:
            inputs = inputs.float().to(device)
            labels = labels.float().to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        assert not np.any(np.isnan(outputs.tolist()))
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        train_loss += loss.item()
        if i % 100 == 99:    # print every 100 mini-batches
            print('[%d, %5d] loss: %.6f' %
                  (epoch + 1, i + 1, train_loss / i))


    # Validation
    tp = 0.0
    tn = 0.0
    fp = 0.0
    fn = 0.0
    epsilon = 1e-7
    model.eval()     # Optional when not using Model Specific layer
    for i, data in enumerate(validloader, 0):
        inputs, labels = data

        # Convert to float
        if device == "cpu":
            inputs = inputs.float()
            labels = labels.float()
        else:
            inputs = inputs.float().to(device)
            labels = labels.float().to(device)
            
        target = nn.Sigmoid()(model(inputs))
        # loss = criterion(target,labels)
        tp += (labels * target).sum(dim=0).to(torch.float32).item()
        tn += ((1 - labels) * (1 - target)).sum(dim=0).to(torch.float32).item()
        fp += ((1 - labels) * target).sum(dim=0).to(torch.float32).item()
        fn += (labels * (1 - target)).sum(dim=0).to(torch.float32).item()
        # f1_loss += f1(target,labels).item()
        # tss_loss += tss(target,labels).item()
        # valid_loss = loss.item() * inputs.size(0)

    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)

    f1 = 2 * (precision*recall) / (precision + recall + epsilon)
    tss = tp / (tp + fn + epsilon) - fp / (fp + tn + epsilon)
    acc = (tp + tn) / (tp + tn + fp + fn)
    print(f'Epoch {epoch+1} \t Training Loss: {train_loss / len(trainloader)} \t F1: {f1 } \t TSS: {tss} \t Accuracy: {acc}')
    
    train_loss_list.append(train_loss)
    valid_loss_list.append((f1, tss))

    # Save model
    if min_valid_loss < f1:
        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{f1:.6f}) \t Saving The Model')
        min_valid_loss = f1
        # Saving State Dict
        torch.save(model.state_dict(), 'saved_model.pth')

print('Finished Training')

# Plot train_loss and valid_loss
# plt.plot(range(1,len(train_loss_list)+1),train_loss_list,'bo',label='Train Loss')
# plt.plot(range(1,len(valid_loss_list)+1),[i[0] for i in valid_loss_list],'r',label='F1 Loss')
# plt.plot(range(1,len(valid_loss_list)+1),[i[1] for i in valid_loss_list],'r',label='TSS Loss')
# plt.legend()
# plt.show()
